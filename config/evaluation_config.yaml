# GyroDiagnostics Evaluation Configuration

# Task Configuration
task:
  epochs: 1  # standard 1, best 6x
  turns: 6   # at least 3, best 6x
  message_limit: 20  
  time_limit: 3600  # 1 hour safety limit (seconds)
  token_limit: 50000  # Prevent runaway generation
  
  # Generation parameters (safety defaults)
  temperature: 0.7  # Balanced creativity vs consistency
  top_p: 0.9  # Nucleus sampling threshold
  top_k: 50  # Top-k sampling limit
  max_tokens: 2048  # Per-response token limit
  
  # Error tolerance configuration
  fail_on_error: 0.1  # Allow up to 10% of samples to have errors
  retry_on_error: 1   # Retry failed samples once
  
  # Production configuration (more conservative)
  # fail_on_error: 0.05  # Allow 5% failure rate for transient issues
  # retry_on_error: 2     # Retry failed samples twice

# Scoring Configuration
scoring:
  weights:
    structure: 0.4
    behavior: 0.4
    specialization: 0.2
  
  level_maximums:
    structure: 50  # 5 metrics × 10 points
    behavior: 60   # 6 metrics × 10 points
    specialization: 20  # 2 metrics × 10 points

# Balance Horizon Configuration
balance_horizon:
  # Reference time constants for Balance Horizon normalization (minutes)
  # These should be calibrated from pilot runs for each challenge type
  reference_times:
    formal: 15.0      # Expected median duration for formal challenges
    normative: 18.0   # Expected median duration for normative challenges  
    procedural: 12.0  # Expected median duration for procedural challenges
    strategic: 20.0   # Expected median duration for strategic challenges
    epistemic: 16.0   # Expected median duration for epistemic challenges
    default: 15.0     # Fallback for unknown challenge types
  
  # Validation bounds
  theoretical_max_horizon: 0.20  # Practical upper bound
  horizon_valid_min: 0.05       # Minimum reasonable threshold

# Challenge Types
challenges:
  - formal
  - normative
  - procedural
  - strategic
  - epistemic

# Model Configuration Examples
models:
  # Models being evaluated (the subjects under test)
  primary:
    - "openai/gpt-4o"
    - "anthropic/claude-3-opus-20240229"
    - "anthropic/claude-3-sonnet-20240229"
  
  # Judge models for scoring (must be different from primary models)
  judge:
    - "anthropic/claude-3-haiku-20240307"  # Fast, cost-effective judge
    - "openai/gpt-4o-mini"                 # Alternative judge option
  
  # IMPORTANT: Judge models must be separate from evaluated models
  # to avoid self-evaluation bias and maintain scoring integrity

# Logging
logging:
  default_dir: "./logs"
  save_transcripts: true
  save_scores: true